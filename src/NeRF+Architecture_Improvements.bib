@InProceedings{Niemeyer2021Regnerf,
          author    = {Michael Niemeyer and Jonathan T. Barron and Ben Mildenhall and Mehdi S. M. Sajjadi and Andreas Geiger and Noha Radwan},  
          title     = {RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs},
	  url = {https://arxiv.org/abs/2112.00724},
          booktitle = {Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
          year      = {2022},
	  	 note = {Project Page: https://m-niemeyer.github.io/regnerf/index.html}
        }
	

@article{kuang2021neroic,
          author = {Kuang, Zhengfei and Olszewski, Kyle and Chai, Menglei and Huang, Zeng and Achlioptas, Panos and Tulyakov, Sergey},
          title = {{NeROIC}: Neural Object Capture and Rendering from Online Image Collections},
          journal = {Computing Research Repository (CoRR)},
          volume = {abs/2201.02533},
          year = {2022},
		  note = {Project Page: https://formyfamily.github.io/NeROIC, Code: https://github.com/snap-research/NeROIC}
          }

@article{wimbauer_-rendering_2022,
	title = {De-rendering {3D} {Objects} in the {Wild}},
	url = {http://arxiv.org/abs/2201.02279},
	abstract = {With increasing focus on augmented and virtual reality applications (XR) comes the demand for algorithms that can lift objects from images and videos into representations that are suitable for a wide variety of related 3D tasks. Large-scale deployment of XR devices and applications means that we cannot solely rely on supervised learning, as collecting and annotating data for the unlimited variety of objects in the real world is infeasible. We present a weakly supervised method that is able to decompose a single image of an object into shape (depth and normals), material (albedo, reflectivity and shininess) and global lighting parameters. For training, the method only relies on a rough initial shape estimate of the training objects to bootstrap the learning process. This shape supervision can come for example from a pretrained depth network or - more generically - from a traditional structure-from-motion pipeline. In our experiments, we show that the method can successfully de-render 2D images into a decomposed 3D representation and generalizes to unseen object categories. Since in-the-wild evaluation is difficult due to the lack of ground truth data, we also introduce a photo-realistic synthetic test set that allows for quantitative evaluation.},
	urldate = {2022-01-23},
	journal = {arXiv:2201.02279 [cs]},
	author = {Wimbauer, Felix and Wu, Shangzhe and Rupprecht, Christian},
	month = jan,
	year = {2022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 2201.02279},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/SVVVNB5C/Wimbauer et al. - 2022 - De-rendering 3D Objects in the Wild.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/E3FGZTN2/2201.html:text/html},
}

@article{kim_infonerf_2021,
	title = {{InfoNeRF}: {Ray} {Entropy} {Minimization} for {Few}-{Shot} {Neural} {Volume} {Rendering}},
	shorttitle = {{InfoNeRF}},
	url = {http://arxiv.org/abs/2112.15399},
	abstract = {We present an information-theoretic regularization technique for few-shot novel view synthesis based on neural implicit representation. The proposed approach minimizes potential reconstruction inconsistency that happens due to insufficient viewpoints by imposing the entropy constraint of the density in each ray. In addition, to alleviate the potential degenerate issue when all training images are acquired from almost redundant viewpoints, we further incorporate the spatially smoothness constraint into the estimated images by restricting information gains from a pair of rays with slightly different viewpoints. The main idea of our algorithm is to make reconstructed scenes compact along individual rays and consistent across rays in the neighborhood. The proposed regularizers can be plugged into most of existing neural volume rendering techniques based on NeRF in a straightforward way. Despite its simplicity, we achieve consistently improved performance compared to existing neural view synthesis methods by large margins on multiple standard benchmarks. Our project website is available at {\textbackslash}textbackslashurl\{http://cvlab.snu.ac.kr/research/InfoNeRF\}.},
	urldate = {2022-01-23},
	journal = {arXiv:2112.15399 [cs, eess]},
	author = {Kim, Mijeong and Seo, Seonguk and Han, Bohyung},
	month = dec,
	year = {2021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {arXiv: 2112.15399},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/SWBHFCWU/Kim et al. - 2021 - InfoNeRF Ray Entropy Minimization for Few-Shot Ne.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/IFTRRP9N/2112.html:text/html},
}

@inproceedings{yoonwoo_jeong_self-calibrating_2021,
	title = {Self-{Calibrating} {Neural} {Radiance} {Fields}},
	booktitle = {{ICCV}},
	author = {Yoonwoo Jeong, Seokjun Ahn, Christopehr Choy, Animashree Anandkumar, Minsu Cho,  and Park, Jaesik},
	year = {2021},
}

@article{xiangli_citynerf_2021,
	title = {{CityNeRF}: {Building} {NeRF} at {City} {Scale}},
	journal = {arXiv preprint arXiv:2112.05504},
	author = {Xiangli, Yuanbo and Xu, Linning and Pan, Xingang and Zhao, Nanxuan and Rao, Anyi and Theobalt, Christian and Dai, Bo and Lin, Dahua},
	year = {2021},
}

@article{tancik_block-nerf_2022,
	title = {Block-{NeRF}: {Scalable} {Large} {Scene} {Neural} {View} {Synthesis}},
	journal = {arXiv},
	author = {Tancik, Matthew and Casser, Vincent and Yan, Xinchen and Pradhan, Sabeek and Mildenhall, Ben and Srinivasan, Pratul and Barron, Jonathan T. and Kretzschmar, Henrik},
	year = {2022},
}

@misc{rematas_sharf_2021,
	title = {{ShaRF}: {Shape}-conditioned {Radiance} {Fields} from a {Single} {View}},
	author = {Rematas, Konstantinos and Martin-Brualla, Ricardo and Ferrari, Vittorio},
	year = {2021},
	annote = {\_eprint: 2102.08860},
}

@misc{kaya_neural_2021,
	title = {Neural {Radiance} {Fields} {Approach} to {Deep} {Multi}-{View} {Photometric} {Stereo}},
	author = {Kaya, Berk and Kumar, Suryansh and Sarno, Francesco and Ferrari, Vittorio and Gool, Luc Van},
	year = {2021},
	annote = {\_eprint: 2110.05594},
}

@article{xu_point-nerf_2022,
	title = {Point-{NeRF}: {Point}-based {Neural} {Radiance} {Fields}},
	journal = {arXiv preprint arXiv:2201.08845},
	author = {Xu, Qiangeng and Xu, Zexiang and Philip, Julien and Bi, Sai and Shu, Zhixin and Sunkavalli, Kalyan and Neumann, Ulrich},
	year = {2022},
}

@article{xie_fig-nerf_2021,
	title = {{FiG}-{NeRF}: {Figure}-{Ground} {Neural} {Radiance} {Fields} for {3D} {Object} {Category} {Modelling}},
	shorttitle = {{FiG}-{NeRF}},
	url = {http://arxiv.org/abs/2104.08418},
	abstract = {We investigate the use of Neural Radiance Fields (NeRF) to learn high quality 3D object category models from collections of input images. In contrast to previous work, we are able to do this whilst simultaneously separating foreground objects from their varying backgrounds. We achieve this via a 2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a geometrically constant background and a deformable foreground that represents the object category. We show that this method can learn accurate 3D object category models using only photometric supervision and casually captured images of the objects. Additionally, our 2-part decomposition allows the model to perform accurate and crisp amodal segmentation. We quantitatively evaluate our method with view synthesis and image ﬁdelity metrics, using synthetic, lab-captured, and in-the-wild data. Our results demonstrate convincing 3D object category modelling that exceed the performance of existing methods.},
	language = {en},
	urldate = {2021-09-25},
	journal = {arXiv:2104.08418 [cs]},
	author = {Xie, Christopher and Park, Keunhong and Martin-Brualla, Ricardo and Brown, Matthew},
	month = apr,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition},
	annote = {ZSCC: 0000001 arXiv: 2104.08418},
	file = {Xie et al. - 2021 - FiG-NeRF Figure-Ground Neural Radiance Fields for.pdf:/home/jack/Zotero/storage/3M4QMGRS/Xie et al. - 2021 - FiG-NeRF Figure-Ground Neural Radiance Fields for.pdf:application/pdf},
}

@article{yu_plenoctrees_2021,
	title = {{PlenOctrees} for {Real}-time {Rendering} of {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2103.14024},
	abstract = {We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800×800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacriﬁcing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve viewdependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu. net/plenoctrees.},
	language = {en},
	urldate = {2021-09-25},
	journal = {arXiv:2103.14024 [cs]},
	author = {Yu, Alex and Li, Ruilong and Tancik, Matthew and Li, Hao and Ng, Ren and Kanazawa, Angjoo},
	month = aug,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: ICCV 2021 (Oral)},
	annote = {ZSCC: 0000012 arXiv: 2103.14024},
	file = {Yu et al. - 2021 - PlenOctrees for Real-time Rendering of Neural Radi.pdf:/home/jack/Zotero/storage/4UDQVPAE/Yu et al. - 2021 - PlenOctrees for Real-time Rendering of Neural Radi.pdf:application/pdf},
}

@inproceedings{mildenhall_nerf_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	isbn = {978-3-030-58452-8},
	shorttitle = {{NeRF}},
	doi = {10/gj826m},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ,ϕ)(θ,ϕ)({\textbackslash}textbackslashtextbackslashtheta ,{\textbackslash}textbackslashtextbackslashphi )) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {3D deep learning, Image-based rendering, Scene representation, View synthesis, Volume rendering},
	pages = {405--421},
	annote = {ZSCC: NoCitationData[s0]},
	file = {Submitted Version:/home/jack/Zotero/storage/NWYLSFAT/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf},
}

@inproceedings{yu_pixelnerf_2021,
	title = {{pixelNeRF}: {Neural} {Radiance} {Fields} {From} {One} or {Few} {Images}},
	shorttitle = {{pixelNeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Yu_pixelNeRF_Neural_Radiance_Fields_From_One_or_Few_Images_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-09-25},
	author = {Yu, Alex and Ye, Vickie and Tancik, Matthew and Kanazawa, Angjoo},
	year = {2021},
	pages = {4578--4587},
	annote = {ZSCC: 0000041},
	file = {Full Text PDF:/home/jack/Zotero/storage/JANH35S9/Yu et al. - 2021 - pixelNeRF Neural Radiance Fields From One or Few .pdf:application/pdf;Snapshot:/home/jack/Zotero/storage/2LMSGYRR/Yu_pixelNeRF_Neural_Radiance_Fields_From_One_or_Few_Images_CVPR_2021_paper.html:text/html},
}

@inproceedings{martin-brualla_nerf_2021,
	title = {{NeRF} in the {Wild}: {Neural} {Radiance} {Fields} for {Unconstrained} {Photo} {Collections}},
	shorttitle = {{NeRF} in the {Wild}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Martin-Brualla_NeRF_in_the_Wild_Neural_Radiance_Fields_for_Unconstrained_Photo_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-09-25},
	author = {Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi S. M. and Barron, Jonathan T. and Dosovitskiy, Alexey and Duckworth, Daniel},
	year = {2021},
	pages = {7210--7219},
	annote = {ZSCC: 0000082},
	file = {Full Text PDF:/home/jack/Zotero/storage/ET6FIPMJ/Martin-Brualla et al. - 2021 - NeRF in the Wild Neural Radiance Fields for Uncon.pdf:application/pdf;Snapshot:/home/jack/Zotero/storage/PYVPQEEI/Martin-Brualla_NeRF_in_the_Wild_Neural_Radiance_Fields_for_Unconstrained_Photo_CVPR_2021_paper.html:text/html},
}

@article{yen-chen_inerf_2021,
	title = {{INeRF}: {Inverting} {Neural} {Radiance} {Fields} for {Pose} {Estimation}},
	shorttitle = {{INeRF}},
	url = {http://arxiv.org/abs/2012.05877},
	abstract = {We present iNeRF, a framework that performs mesh-free pose estimation by "inverting" a Neural RadianceField (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis - synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation - given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.},
	urldate = {2021-09-25},
	journal = {arXiv:2012.05877 [cs]},
	author = {Yen-Chen, Lin and Florence, Pete and Barron, Jonathan T. and Rodriguez, Alberto and Isola, Phillip and Lin, Tsung-Yi},
	month = aug,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: IROS 2021, Website: http://yenchenlin.me/inerf/},
	annote = {ZSCC: NoCitationData[s0] arXiv: 2012.05877},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/CT7XRVWR/Yen-Chen et al. - 2021 - INeRF Inverting Neural Radiance Fields for Pose E.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/93ZRLS7C/2012.html:text/html},
}

@article{gao_portrait_2021,
	title = {Portrait {Neural} {Radiance} {Fields} from a {Single} {Image}},
	url = {http://arxiv.org/abs/2012.05903},
	abstract = {We present a method for estimating Neural Radiance Fields (NeRF) from a single headshot portrait. While NeRF has demonstrated high-quality view synthesis, it requires multiple images of static scenes and thus impractical for casual captures and moving subjects. In this work, we propose to pretrain the weights of a multilayer perceptron (MLP), which implicitly models the volumetric density and colors, with a meta-learning framework using a light stage portrait dataset. To improve the generalization to unseen faces, we train the MLP in the canonical coordinate space approximated by 3D face morphable models. We quantitatively evaluate the method using controlled captures and demonstrate the generalization to real portrait images, showing favorable results against state-of-the-arts.},
	urldate = {2021-09-25},
	journal = {arXiv:2012.05903 [cs]},
	author = {Gao, Chen and Shih, Yichang and Lai, Wei-Sheng and Liang, Chia-Kai and Huang, Jia-Bin},
	month = apr,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project webpage: https://portrait-nerf.github.io/},
	annote = {ZSCC: NoCitationData[s0] arXiv: 2012.05903},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/7NW6B9MG/Gao et al. - 2021 - Portrait Neural Radiance Fields from a Single Imag.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/JMEHQ4CL/2012.html:text/html},
}

@article{lin_barf_2021,
	title = {{BARF}: {Bundle}-{Adjusting} {Neural} {Radiance} {Fields}},
	shorttitle = {{BARF}},
	url = {http://arxiv.org/abs/2104.06405},
	abstract = {Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses – the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na{\textbackslash}textbackslashtextbackslash"ively applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.},
	urldate = {2021-09-25},
	journal = {arXiv:2104.06405 [cs]},
	author = {Lin, Chen-Hsuan and Ma, Wei-Chiu and Torralba, Antonio and Lucey, Simon},
	month = aug,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: Accepted to ICCV 2021 as oral presentation (project page \& code: https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF)},
	annote = {ZSCC: 0000003 arXiv: 2104.06405},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/JE6EBEIX/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/C7UT3SLU/2104.html:text/html},
}

@article{zhang_nerf_2020,
	title = {{NeRF}++: {Analyzing} and {Improving} {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}++},
	url = {http://arxiv.org/abs/2010.07492},
	abstract = {Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360◦ capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF ﬁts multilayer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we ﬁrst remark on radiance ﬁelds and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF’s success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360◦ captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis ﬁdelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.},
	language = {en},
	urldate = {2021-09-25},
	journal = {arXiv:2010.07492 [cs]},
	author = {Zhang, Kai and Riegler, Gernot and Snavely, Noah and Koltun, Vladlen},
	month = oct,
	year = {2020},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Code is available at https://github.com/Kai-46/nerfplusplus; fix a minor formatting issue in Fig. 4},
	annote = {ZSCC: NoCitationData[s0] arXiv: 2010.07492},
	file = {Zhang et al. - 2020 - NeRF++ Analyzing and Improving Neural Radiance Fi.pdf:/home/jack/Zotero/storage/XB9MGTMK/Zhang et al. - 2020 - NeRF++ Analyzing and Improving Neural Radiance Fi.pdf:application/pdf},
}

@article{reiser_kilonerf_2021,
	title = {{KiloNeRF}: {Speeding} up {Neural} {Radiance} {Fields} with {Thousands} of {Tiny} {MLPs}},
	shorttitle = {{KiloNeRF}},
	url = {http://arxiv.org/abs/2103.13744},
	abstract = {NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.},
	urldate = {2021-09-25},
	journal = {arXiv:2103.13744 [cs]},
	author = {Reiser, Christian and Peng, Songyou and Liao, Yiyi and Geiger, Andreas},
	month = aug,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICCV 2021. Code, pretrained models and an interactive viewer are available at https://github.com/creiser/kilonerf/},
	annote = {ZSCC: 0000006 arXiv: 2103.13744},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/S7G3N9HV/Reiser et al. - 2021 - KiloNeRF Speeding up Neural Radiance Fields with .pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/YCNWFGTS/2103.html:text/html},
}

@inproceedings{rebain_derf_2021,
	title = {{DeRF}: {Decomposed} {Radiance} {Fields}},
	shorttitle = {{DeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Rebain_DeRF_Decomposed_Radiance_Fields_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-09-25},
	author = {Rebain, Daniel and Jiang, Wei and Yazdani, Soroosh and Li, Ke and Yi, Kwang Moo and Tagliasacchi, Andrea},
	year = {2021},
	pages = {14153--14161},
	annote = {ZSCC: 0000017},
	file = {Full Text PDF:/home/jack/Zotero/storage/3IGZ9P5N/Rebain et al. - 2021 - DeRF Decomposed Radiance Fields.pdf:application/pdf;Snapshot:/home/jack/Zotero/storage/DW5X8L7G/Rebain_DeRF_Decomposed_Radiance_Fields_CVPR_2021_paper.html:text/html},
}

@article{barron_mip-nerf_2021,
	title = {Mip-{NeRF}: {A} {Multiscale} {Representation} for {Anti}-{Aliasing} {Neural} {Radiance} {Fields}},
	shorttitle = {Mip-{NeRF}},
	url = {http://arxiv.org/abs/2103.13415},
	abstract = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-NeRF" (a la "mipmap"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.},
	urldate = {2021-09-25},
	journal = {arXiv:2103.13415 [cs]},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
	month = aug,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {ZSCC: 0000003 arXiv: 2103.13415},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/U4P2XYSN/Barron et al. - 2021 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/YMNW6YNA/2103.html:text/html},
}

@article{hedman_baking_2021,
	title = {Baking {Neural} {Radiance} {Fields} for {Real}-{Time} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2103.14645},
	abstract = {Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. "bake") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.},
	urldate = {2021-09-25},
	journal = {arXiv:2103.14645 [cs]},
	author = {Hedman, Peter and Srinivasan, Pratul P. and Mildenhall, Ben and Barron, Jonathan T. and Debevec, Paul},
	month = mar,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: Project page: https://nerf.live},
	annote = {ZSCC: 0000002 arXiv: 2103.14645},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/QT2L9Q9Z/Hedman et al. - 2021 - Baking Neural Radiance Fields for Real-Time View S.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/REXBSYHG/2103.html:text/html},
}

@article{wang_nerf_2021,
	title = {{NeRF}–: {Neural} {Radiance} {Fields} {Without} {Known} {Camera} {Parameters}},
	shorttitle = {{NeRF}–},
	url = {http://arxiv.org/abs/2102.07064},
	abstract = {This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses and intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recently gained popularity due to its remarkable synthesis quality. Existing NeRF-based approaches assume that the camera parameters associated with each input image are either directly accessible at training, or can be accurately estimated with conventional techniques based on correspondences, such as Structure-from-Motion. In this work, we propose an end-to-end framework, termed NeRF–, for training NeRF models given only RGB images, without pre-computed camera parameters. Specifically, we show that the camera parameters, including both intrinsics and extrinsics, can be automatically discovered via joint optimisation during the training of the NeRF model. On the standard LLFF benchmark, our model achieves comparable novel view synthesis results compared to the baseline trained with COLMAP pre-computed camera parameters. We also conduct extensive analyses to understand the model behaviour under different camera trajectories, and show that in scenarios where COLMAP fails, our model still produces robust results.},
	language = {en},
	urldate = {2021-09-25},
	journal = {arXiv:2102.07064 [cs]},
	author = {Wang, Zirui and Wu, Shangzhe and Xie, Weidi and Chen, Min and Prisacariu, Victor Adrian},
	month = feb,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: project page see nerfmm.active.vision},
	annote = {ZSCC: 0000010 arXiv: 2102.07064},
	file = {Wang et al. - 2021 - NeRF-- Neural Radiance Fields Without Known Camer.pdf:/home/jack/Zotero/storage/3ZWIX5H6/Wang et al. - 2021 - NeRF-- Neural Radiance Fields Without Known Camer.pdf:application/pdf},
}

@article{li_mine_2021,
	title = {{MINE}: {Towards} {Continuous} {Depth} {MPI} with {NeRF} for {Novel} {View} {Synthesis}},
	shorttitle = {{MINE}},
	url = {http://arxiv.org/abs/2103.14910},
	abstract = {In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE},
	urldate = {2021-10-11},
	journal = {arXiv:2103.14910 [cs]},
	author = {Li, Jiaxin and Feng, Zijian and She, Qi and Ding, Henghui and Wang, Changhu and Lee, Gim Hee},
	month = jul,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
	annote = {Comment: ICCV 2021. Main paper and supplementary materials},
	annote = {ZSCC: 0000000 arXiv: 2103.14910},
}
