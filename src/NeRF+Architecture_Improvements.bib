@InProceedings{Niemeyer2021Regnerf,
          author    = {Michael Niemeyer and Jonathan T. Barron and Ben Mildenhall and Mehdi S. M. Sajjadi and Andreas Geiger and Noha Radwan},  
          title     = {RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs},
	  url =     {https://arxiv.org/abs/2112.00724},
          booktitle = {Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
          year      = {2022},
	  	 note = {Project Page: https://m-niemeyer.github.io/regnerf/index.html}
        }

@article{kuang2021neroic,
          author = {Kuang, Zhengfei and Olszewski, Kyle and Chai, Menglei and Huang, Zeng and Achlioptas, Panos and Tulyakov, Sergey},
          title = {{NeROIC}: Neural Object Capture and Rendering from Online Image Collections},
          journal = {Computing Research Repository (CoRR)},
          volume = {abs/2201.02533},
          year = {2022},
		  note = {Project Page: https://formyfamily.github.io/NeROIC, Code: https://github.com/snap-research/NeROIC}
          }

@article{wimbauer_-rendering_2022,
	title = {De-rendering {3D} {Objects} in the {Wild}},
	url = {http://arxiv.org/abs/2201.02279},
	abstract = {With increasing focus on augmented and virtual reality applications (XR) comes the demand for algorithms that can lift objects from images and videos into representations that are suitable for a wide variety of related 3D tasks. Large-scale deployment of XR devices and applications means that we cannot solely rely on supervised learning, as collecting and annotating data for the unlimited variety of objects in the real world is infeasible. We present a weakly supervised method that is able to decompose a single image of an object into shape (depth and normals), material (albedo, reflectivity and shininess) and global lighting parameters. For training, the method only relies on a rough initial shape estimate of the training objects to bootstrap the learning process. This shape supervision can come for example from a pretrained depth network or - more generically - from a traditional structure-from-motion pipeline. In our experiments, we show that the method can successfully de-render 2D images into a decomposed 3D representation and generalizes to unseen object categories. Since in-the-wild evaluation is difficult due to the lack of ground truth data, we also introduce a photo-realistic synthetic test set that allows for quantitative evaluation.},
	urldate = {2022-01-23},
	journal = {arXiv:2201.02279 [cs]},
	author = {Wimbauer, Felix and Wu, Shangzhe and Rupprecht, Christian},
	month = jan,
	year = {2022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 2201.02279},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/SVVVNB5C/Wimbauer et al. - 2022 - De-rendering 3D Objects in the Wild.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/E3FGZTN2/2201.html:text/html},
}

@article{kim_infonerf_2021,
	title = {{InfoNeRF}: {Ray} {Entropy} {Minimization} for {Few}-{Shot} {Neural} {Volume} {Rendering}},
	shorttitle = {{InfoNeRF}},
	url = {http://arxiv.org/abs/2112.15399},
	abstract = {We present an information-theoretic regularization technique for few-shot novel view synthesis based on neural implicit representation. The proposed approach minimizes potential reconstruction inconsistency that happens due to insufficient viewpoints by imposing the entropy constraint of the density in each ray. In addition, to alleviate the potential degenerate issue when all training images are acquired from almost redundant viewpoints, we further incorporate the spatially smoothness constraint into the estimated images by restricting information gains from a pair of rays with slightly different viewpoints. The main idea of our algorithm is to make reconstructed scenes compact along individual rays and consistent across rays in the neighborhood. The proposed regularizers can be plugged into most of existing neural volume rendering techniques based on NeRF in a straightforward way. Despite its simplicity, we achieve consistently improved performance compared to existing neural view synthesis methods by large margins on multiple standard benchmarks. Our project website is available at {\textbackslash}textbackslashurl\{http://cvlab.snu.ac.kr/research/InfoNeRF\}.},
	urldate = {2022-01-23},
	journal = {arXiv:2112.15399 [cs, eess]},
	author = {Kim, Mijeong and Seo, Seonguk and Han, Bohyung},
	month = dec,
	year = {2021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {arXiv: 2112.15399},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/SWBHFCWU/Kim et al. - 2021 - InfoNeRF Ray Entropy Minimization for Few-Shot Ne.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/IFTRRP9N/2112.html:text/html},
}

@inproceedings{yoonwoo_jeong_self-calibrating_2021,
	title = {Self-{Calibrating} {Neural} {Radiance} {Fields}},
	booktitle = {{ICCV}},
	author = {Yoonwoo Jeong, Seokjun Ahn, Christopehr Choy, Animashree Anandkumar, Minsu Cho,  and Park, Jaesik},
	year = {2021},
}

@article{xiangli_citynerf_2021,
	title = {{CityNeRF}: {Building} {NeRF} at {City} {Scale}},
	journal = {arXiv preprint arXiv:2112.05504},
	author = {Xiangli, Yuanbo and Xu, Linning and Pan, Xingang and Zhao, Nanxuan and Rao, Anyi and Theobalt, Christian and Dai, Bo and Lin, Dahua},
	year = {2021},
}

@article{tancik_block-nerf_2022,
	title = {Block-{NeRF}: {Scalable} {Large} {Scene} {Neural} {View} {Synthesis}},
	journal = {arXiv},
	author = {Tancik, Matthew and Casser, Vincent and Yan, Xinchen and Pradhan, Sabeek and Mildenhall, Ben and Srinivasan, Pratul and Barron, Jonathan T. and Kretzschmar, Henrik},
	year = {2022},
}

@misc{rematas_sharf_2021,
	title = {{ShaRF}: {Shape}-conditioned {Radiance} {Fields} from a {Single} {View}},
	author = {Rematas, Konstantinos and Martin-Brualla, Ricardo and Ferrari, Vittorio},
	year = {2021},
	annote = {\_eprint: 2102.08860},
}

@misc{kaya_neural_2021,
	title = {Neural {Radiance} {Fields} {Approach} to {Deep} {Multi}-{View} {Photometric} {Stereo}},
	author = {Kaya, Berk and Kumar, Suryansh and Sarno, Francesco and Ferrari, Vittorio and Gool, Luc Van},
	year = {2021},
	annote = {\_eprint: 2110.05594},
}

@article{xu_point-nerf_2022,
	title = {Point-{NeRF}: {Point}-based {Neural} {Radiance} {Fields}},
	journal = {arXiv preprint arXiv:2201.08845},
	author = {Xu, Qiangeng and Xu, Zexiang and Philip, Julien and Bi, Sai and Shu, Zhixin and Sunkavalli, Kalyan and Neumann, Ulrich},
	year = {2022},
}

@article{xie_fig-nerf_2021,
	title = {{FiG}-{NeRF}: {Figure}-{Ground} {Neural} {Radiance} {Fields} for {3D} {Object} {Category} {Modelling}},
	shorttitle = {{FiG}-{NeRF}},
	url = {http://arxiv.org/abs/2104.08418},
	abstract = {We investigate the use of Neural Radiance Fields (NeRF) to learn high quality 3D object category models from collections of input images. In contrast to previous work, we are able to do this whilst simultaneously separating foreground objects from their varying backgrounds. We achieve this via a 2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a geometrically constant background and a deformable foreground that represents the object category. We show that this method can learn accurate 3D object category models using only photometric supervision and casually captured images of the objects. Additionally, our 2-part decomposition allows the model to perform accurate and crisp amodal segmentation. We quantitatively evaluate our method with view synthesis and image ﬁdelity metrics, using synthetic, lab-captured, and in-the-wild data. Our results demonstrate convincing 3D object category modelling that exceed the performance of existing methods.},
	language = {en},
	urldate = {2021-09-25},
	journal = {arXiv:2104.08418 [cs]},
	author = {Xie, Christopher and Park, Keunhong and Martin-Brualla, Ricardo and Brown, Matthew},
	month = apr,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition},
	annote = {ZSCC: 0000001 arXiv: 2104.08418},
	file = {Xie et al. - 2021 - FiG-NeRF Figure-Ground Neural Radiance Fields for.pdf:/home/jack/Zotero/storage/3M4QMGRS/Xie et al. - 2021 - FiG-NeRF Figure-Ground Neural Radiance Fields for.pdf:application/pdf},
}

@article{yu_plenoctrees_2021,
	title = {{PlenOctrees} for {Real}-time {Rendering} of {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2103.14024},
	abstract = {We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800×800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacriﬁcing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve viewdependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu. net/plenoctrees.},
	language = {en},
	urldate = {2021-09-25},
	journal = {arXiv:2103.14024 [cs]},
	author = {Yu, Alex and Li, Ruilong and Tancik, Matthew and Li, Hao and Ng, Ren and Kanazawa, Angjoo},
	month = aug,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: ICCV 2021 (Oral)},
	annote = {ZSCC: 0000012 arXiv: 2103.14024},
	file = {Yu et al. - 2021 - PlenOctrees for Real-time Rendering of Neural Radi.pdf:/home/jack/Zotero/storage/4UDQVPAE/Yu et al. - 2021 - PlenOctrees for Real-time Rendering of Neural Radi.pdf:application/pdf},
}

@inproceedings{mildenhall_nerf_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	isbn = {978-3-030-58452-8},
	shorttitle = {{NeRF}},
	doi = {10/gj826m},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ,ϕ)(θ,ϕ)({\textbackslash}textbackslashtextbackslashtheta ,{\textbackslash}textbackslashtextbackslashphi )) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {3D deep learning, Image-based rendering, Scene representation, View synthesis, Volume rendering},
	pages = {405--421},
	annote = {ZSCC: NoCitationData[s0]},
	file = {Submitted Version:/home/jack/Zotero/storage/NWYLSFAT/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf},
}

@inproceedings{yu_pixelnerf_2021,
	title = {{pixelNeRF}: {Neural} {Radiance} {Fields} {From} {One} or {Few} {Images}},
	shorttitle = {{pixelNeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Yu_pixelNeRF_Neural_Radiance_Fields_From_One_or_Few_Images_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-09-25},
	author = {Yu, Alex and Ye, Vickie and Tancik, Matthew and Kanazawa, Angjoo},
	year = {2021},
	pages = {4578--4587},
	annote = {ZSCC: 0000041},
	file = {Full Text PDF:/home/jack/Zotero/storage/JANH35S9/Yu et al. - 2021 - pixelNeRF Neural Radiance Fields From One or Few .pdf:application/pdf;Snapshot:/home/jack/Zotero/storage/2LMSGYRR/Yu_pixelNeRF_Neural_Radiance_Fields_From_One_or_Few_Images_CVPR_2021_paper.html:text/html},
}

@inproceedings{martin-brualla_nerf_2021,
	title = {{NeRF} in the {Wild}: {Neural} {Radiance} {Fields} for {Unconstrained} {Photo} {Collections}},
	shorttitle = {{NeRF} in the {Wild}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Martin-Brualla_NeRF_in_the_Wild_Neural_Radiance_Fields_for_Unconstrained_Photo_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-09-25},
	author = {Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi S. M. and Barron, Jonathan T. and Dosovitskiy, Alexey and Duckworth, Daniel},
	year = {2021},
	pages = {7210--7219},
	annote = {ZSCC: 0000082},
	file = {Full Text PDF:/home/jack/Zotero/storage/ET6FIPMJ/Martin-Brualla et al. - 2021 - NeRF in the Wild Neural Radiance Fields for Uncon.pdf:application/pdf;Snapshot:/home/jack/Zotero/storage/PYVPQEEI/Martin-Brualla_NeRF_in_the_Wild_Neural_Radiance_Fields_for_Unconstrained_Photo_CVPR_2021_paper.html:text/html},
}

@article{yen-chen_inerf_2021,
	title = {{INeRF}: {Inverting} {Neural} {Radiance} {Fields} for {Pose} {Estimation}},
	shorttitle = {{INeRF}},
	url = {http://arxiv.org/abs/2012.05877},
	abstract = {We present iNeRF, a framework that performs mesh-free pose estimation by "inverting" a Neural RadianceField (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis - synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation - given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.},
	urldate = {2021-09-25},
	journal = {arXiv:2012.05877 [cs]},
	author = {Yen-Chen, Lin and Florence, Pete and Barron, Jonathan T. and Rodriguez, Alberto and Isola, Phillip and Lin, Tsung-Yi},
	month = aug,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: IROS 2021, Website: http://yenchenlin.me/inerf/},
	annote = {ZSCC: NoCitationData[s0] arXiv: 2012.05877},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/CT7XRVWR/Yen-Chen et al. - 2021 - INeRF Inverting Neural Radiance Fields for Pose E.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/93ZRLS7C/2012.html:text/html},
}

@article{gao_portrait_2021,
	title = {Portrait {Neural} {Radiance} {Fields} from a {Single} {Image}},
	url = {http://arxiv.org/abs/2012.05903},
	abstract = {We present a method for estimating Neural Radiance Fields (NeRF) from a single headshot portrait. While NeRF has demonstrated high-quality view synthesis, it requires multiple images of static scenes and thus impractical for casual captures and moving subjects. In this work, we propose to pretrain the weights of a multilayer perceptron (MLP), which implicitly models the volumetric density and colors, with a meta-learning framework using a light stage portrait dataset. To improve the generalization to unseen faces, we train the MLP in the canonical coordinate space approximated by 3D face morphable models. We quantitatively evaluate the method using controlled captures and demonstrate the generalization to real portrait images, showing favorable results against state-of-the-arts.},
	urldate = {2021-09-25},
	journal = {arXiv:2012.05903 [cs]},
	author = {Gao, Chen and Shih, Yichang and Lai, Wei-Sheng and Liang, Chia-Kai and Huang, Jia-Bin},
	month = apr,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project webpage: https://portrait-nerf.github.io/},
	annote = {ZSCC: NoCitationData[s0] arXiv: 2012.05903},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/7NW6B9MG/Gao et al. - 2021 - Portrait Neural Radiance Fields from a Single Imag.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/JMEHQ4CL/2012.html:text/html},
}

@article{lin_barf_2021,
	title = {{BARF}: {Bundle}-{Adjusting} {Neural} {Radiance} {Fields}},
	shorttitle = {{BARF}},
	url = {http://arxiv.org/abs/2104.06405},
	abstract = {Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses – the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na{\textbackslash}textbackslashtextbackslash"ively applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.},
	urldate = {2021-09-25},
	journal = {arXiv:2104.06405 [cs]},
	author = {Lin, Chen-Hsuan and Ma, Wei-Chiu and Torralba, Antonio and Lucey, Simon},
	month = aug,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: Accepted to ICCV 2021 as oral presentation (project page \& code: https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF)},
	annote = {ZSCC: 0000003 arXiv: 2104.06405},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/JE6EBEIX/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/C7UT3SLU/2104.html:text/html},
}

@article{zhang_nerf_2020,
	title = {{NeRF}++: {Analyzing} and {Improving} {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}++},
	url = {http://arxiv.org/abs/2010.07492},
	abstract = {Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360◦ capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF ﬁts multilayer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we ﬁrst remark on radiance ﬁelds and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF’s success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360◦ captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis ﬁdelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.},
	language = {en},
	urldate = {2021-09-25},
	journal = {arXiv:2010.07492 [cs]},
	author = {Zhang, Kai and Riegler, Gernot and Snavely, Noah and Koltun, Vladlen},
	month = oct,
	year = {2020},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Code is available at https://github.com/Kai-46/nerfplusplus; fix a minor formatting issue in Fig. 4},
	annote = {ZSCC: NoCitationData[s0] arXiv: 2010.07492},
	file = {Zhang et al. - 2020 - NeRF++ Analyzing and Improving Neural Radiance Fi.pdf:/home/jack/Zotero/storage/XB9MGTMK/Zhang et al. - 2020 - NeRF++ Analyzing and Improving Neural Radiance Fi.pdf:application/pdf},
}

@article{reiser_kilonerf_2021,
	title = {{KiloNeRF}: {Speeding} up {Neural} {Radiance} {Fields} with {Thousands} of {Tiny} {MLPs}},
	shorttitle = {{KiloNeRF}},
	url = {http://arxiv.org/abs/2103.13744},
	abstract = {NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.},
	urldate = {2021-09-25},
	journal = {arXiv:2103.13744 [cs]},
	author = {Reiser, Christian and Peng, Songyou and Liao, Yiyi and Geiger, Andreas},
	month = aug,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICCV 2021. Code, pretrained models and an interactive viewer are available at https://github.com/creiser/kilonerf/},
	annote = {ZSCC: 0000006 arXiv: 2103.13744},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/S7G3N9HV/Reiser et al. - 2021 - KiloNeRF Speeding up Neural Radiance Fields with .pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/YCNWFGTS/2103.html:text/html},
}

@inproceedings{rebain_derf_2021,
	title = {{DeRF}: {Decomposed} {Radiance} {Fields}},
	shorttitle = {{DeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Rebain_DeRF_Decomposed_Radiance_Fields_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-09-25},
	author = {Rebain, Daniel and Jiang, Wei and Yazdani, Soroosh and Li, Ke and Yi, Kwang Moo and Tagliasacchi, Andrea},
	year = {2021},
	pages = {14153--14161},
	annote = {ZSCC: 0000017},
	file = {Full Text PDF:/home/jack/Zotero/storage/3IGZ9P5N/Rebain et al. - 2021 - DeRF Decomposed Radiance Fields.pdf:application/pdf;Snapshot:/home/jack/Zotero/storage/DW5X8L7G/Rebain_DeRF_Decomposed_Radiance_Fields_CVPR_2021_paper.html:text/html},
}

@article{barron_mip-nerf_2021,
	title = {Mip-{NeRF}: {A} {Multiscale} {Representation} for {Anti}-{Aliasing} {Neural} {Radiance} {Fields}},
	shorttitle = {Mip-{NeRF}},
	url = {http://arxiv.org/abs/2103.13415},
	abstract = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-NeRF" (a la "mipmap"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.},
	urldate = {2021-09-25},
	journal = {arXiv:2103.13415 [cs]},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
	month = aug,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {ZSCC: 0000003 arXiv: 2103.13415},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/U4P2XYSN/Barron et al. - 2021 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/YMNW6YNA/2103.html:text/html},
}

@article{hedman_baking_2021,
	title = {Baking {Neural} {Radiance} {Fields} for {Real}-{Time} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2103.14645},
	abstract = {Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. "bake") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.},
	urldate = {2021-09-25},
	journal = {arXiv:2103.14645 [cs]},
	author = {Hedman, Peter and Srinivasan, Pratul P. and Mildenhall, Ben and Barron, Jonathan T. and Debevec, Paul},
	month = mar,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: Project page: https://nerf.live},
	annote = {ZSCC: 0000002 arXiv: 2103.14645},
	file = {arXiv Fulltext PDF:/home/jack/Zotero/storage/QT2L9Q9Z/Hedman et al. - 2021 - Baking Neural Radiance Fields for Real-Time View S.pdf:application/pdf;arXiv.org Snapshot:/home/jack/Zotero/storage/REXBSYHG/2103.html:text/html},
}

@article{wang_nerf_2021,
	title = {{NeRF}–: {Neural} {Radiance} {Fields} {Without} {Known} {Camera} {Parameters}},
	shorttitle = {{NeRF}–},
	url = {http://arxiv.org/abs/2102.07064},
	abstract = {This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses and intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recently gained popularity due to its remarkable synthesis quality. Existing NeRF-based approaches assume that the camera parameters associated with each input image are either directly accessible at training, or can be accurately estimated with conventional techniques based on correspondences, such as Structure-from-Motion. In this work, we propose an end-to-end framework, termed NeRF–, for training NeRF models given only RGB images, without pre-computed camera parameters. Specifically, we show that the camera parameters, including both intrinsics and extrinsics, can be automatically discovered via joint optimisation during the training of the NeRF model. On the standard LLFF benchmark, our model achieves comparable novel view synthesis results compared to the baseline trained with COLMAP pre-computed camera parameters. We also conduct extensive analyses to understand the model behaviour under different camera trajectories, and show that in scenarios where COLMAP fails, our model still produces robust results.},
	language = {en},
	urldate = {2021-09-25},
	journal = {arXiv:2102.07064 [cs]},
	author = {Wang, Zirui and Wu, Shangzhe and Xie, Weidi and Chen, Min and Prisacariu, Victor Adrian},
	month = feb,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: project page see nerfmm.active.vision},
	annote = {ZSCC: 0000010 arXiv: 2102.07064},
	file = {Wang et al. - 2021 - NeRF-- Neural Radiance Fields Without Known Camer.pdf:/home/jack/Zotero/storage/3ZWIX5H6/Wang et al. - 2021 - NeRF-- Neural Radiance Fields Without Known Camer.pdf:application/pdf},
}

@article{li_mine_2021,
	title = {{MINE}: {Towards} {Continuous} {Depth} {MPI} with {NeRF} for {Novel} {View} {Synthesis}},
	shorttitle = {{MINE}},
	url = {http://arxiv.org/abs/2103.14910},
	abstract = {In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE},
	urldate = {2021-10-11},
	journal = {arXiv:2103.14910 [cs]},
	author = {Li, Jiaxin and Feng, Zijian and She, Qi and Ding, Henghui and Wang, Changhu and Lee, Gim Hee},
	month = jul,
	year = {2021},
	keywords = {⛔ No DOI found, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
	annote = {Comment: ICCV 2021. Main paper and supplementary materials},
	annote = {ZSCC: 0000000 arXiv: 2103.14910},
}



%% Novel ideas
@article{chen2023factor,
  title={Factor Fields: A Unified Framework for Neural Fields and Beyond},
  author={Chen, Anpei and Xu, Zexiang and Wei, Xinyue and Tang, Siyu and Su, Hao and Geiger, Andreas},
  journal={arXiv preprint arXiv:2302.01226},
  year={2023}
}

@inproceedings{
  ma2023image,
  title={Image as Set of Points},
  author={Xu Ma and Yuqian Zhou and Huan Wang and Can Qin and Bin Sun and Chang Liu and Yun Fu},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=awnvqZja69}
}

@inproceedings{mehta2022level,
  title={A level set theory for neural implicit evolution under explicit flows},
  author={Mehta, Ishit and Chandraker, Manmohan and Ramamoorthi, Ravi},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part II},
  pages={711--729},
  year={2022},
  organization={Springer}
}

@article{zhang2022seeing,
  title={Seeing a Rose in Five Thousand Ways},
  author={Zhang, Yunzhi and Wu, Shangzhe and Snavely, Noah and Wu, Jiajun},
  journal={arXiv preprint arXiv:2212.04965},
  year={2022}
}

%% Novel Approach
@article{nguyen2022s4nd,
  title={S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces},
  author={Nguyen, Eric and Goel, Karan and Gu, Albert and Downs, Gordon W and Shah, Preey and Dao, Tri and Baccus, Stephen A and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2210.06583},
  year={2022}
}

@article{dogaru2022sphere,
  title={Sphere-Guided Training of Neural Implicit Surfaces},
  author={Dogaru, Andreea and Ardelean, Andrei Timotei and Ignatyev, Savva and Burnaev, Evgeny and Zakharov, Egor},
  journal={arXiv preprint arXiv:2209.15511},
  year={2022}
}

@article{ma2022totems,
  author    = {Ma, Jingwei and Chai, Lucy and Huh, Minyoung and Wang, Tongzhou and Lim, Ser-Nam and Isola, Phillip and Torralba, Antonio},
  title     = {Totems: Physical Objects for Verifying Visual Integrity},
  journal   = {ECCV},
  year      = {2022},
}

@inproceedings{
yang2022polynomial,
title={Polynomial Neural Fields for Subband Decomposition and Manipulation},
author={Guandao Yang and Sagie Benaim and Varun Jampani and Kyle Genova and Jonathan T. Barron and Thomas Funkhouser and Bharath Hariharan and Serge Belongie},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=juE5ErmZB61}
}

@article{shue20223d,
  title={3D Neural Field Generation using Triplane Diffusion},
  author={Shue, J Ryan and Chan, Eric Ryan and Po, Ryan and Ankner, Zachary and Wu, Jiajun and Wetzstein, Gordon},
  journal={arXiv preprint arXiv:2211.16677},
  year={2022}
}

@article{wang20224k,
  title={4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions},
  author={Wang, Zhongshu and Li, Lingzhi and Shen, Zhen and Shen, Li and Bo, Liefeng},
  journal={arXiv preprint arXiv:2212.04701},
  year={2022}
}

@article{chung2022meil,
  title={MEIL-NeRF: Memory-Efficient Incremental Learning of Neural Radiance Fields},
  author={Chung, Jaeyoung and Lee, Kanggeon and Baik, Sungyong and Lee, Kyoung Mu},
  journal={arXiv preprint arXiv:2212.08328},
  year={2022}
}

@article{siddiqui2022panoptic,
  title={Panoptic Lifting for 3D Scene Understanding with Neural Fields},
  author={Siddiqui, Yawar and Porzi, Lorenzo and Bul{\'o}, Samuel Rota and M{\"u}ller, Norman and Nie{\ss}ner, Matthias and Dai, Angela and Kontschieder, Peter},
  journal={arXiv preprint arXiv:2212.09802},
  year={2022}
}

@article{li2022climatenerf,
  title={ClimateNeRF: Physically-based Neural Rendering for Extreme Climate Synthesis},
  author={Li, Yuan and Lin, Zhi-Hao and Forsyth, David and Huang, Jia-Bin and Wang, Shenlong},
  journal={arXiv e-prints},
  pages={arXiv--2211},
  year={2022}
}

@article{zarzar2022segnerf,
  title={SegNeRF: 3D Part Segmentation with Neural Radiance Fields},
  author={Zarzar, Jesus and Rojas, Sara and Giancola, Silvio and Ghanem, Bernard},
  journal={arXiv preprint arXiv:2211.11215},
  year={2022}
}

@article{guo2022incremental,
  title={Incremental Learning for Neural Radiance Field with Uncertainty-Filtered Knowledge Distillation},
  author={Guo, Mengqi and Li, Chen and Lee, Gim Hee},
  journal={arXiv preprint arXiv:2212.10950},
  year={2022}
}

@article{weder2022removing,
  title={Removing Objects From Neural Radiance Fields},
  author={Weder, Silvan and Garcia-Hernando, Guillermo and Monszpart, Aron and Pollefeys, Marc and Brostow, Gabriel and Firman, Michael and Vicente, Sara},
  journal={arXiv preprint arXiv:2212.11966},
  year={2022}
}

@article{fridovich2023k,
  title={K-planes: Explicit radiance fields in space, time, and appearance},
  author={Fridovich-Keil, Sara and Meanti, Giacomo and Warburg, Frederik and Recht, Benjamin and Kanazawa, Angjoo},
  journal={arXiv preprint arXiv:2301.10241},
  year={2023}
}

@article{reiser2023merf,
  title={Merf: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes},
  author={Reiser, Christian and Szeliski, Richard and Verbin, Dor and Srinivasan, Pratul P and Mildenhall, Ben and Geiger, Andreas and Barron, Jonathan T and Hedman, Peter},
  journal={arXiv preprint arXiv:2302.12249},
  year={2023}
}

@article{yariv2023bakedsdf,
  title={BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis},
  author={Yariv, Lior and Hedman, Peter and Reiser, Christian and Verbin, Dor and Srinivasan, Pratul P and Szeliski, Richard and Barron, Jonathan T and Mildenhall, Ben},
  journal={arXiv preprint arXiv:2302.14859},
  year={2023}
}

@article{zhang2023nerflets,
  title={Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision},
  author={Zhang, Xiaoshuai and Kundu, Abhijit and Funkhouser, Thomas and Guibas, Leonidas and Su, Hao and Genova, Kyle},
  journal={arXiv preprint arXiv:2303.03361},
  year={2023}
}

@article{han2023multiscale,
  title={Multiscale Tensor Decomposition and Rendering Equation Encoding for View Synthesis},
  author={Han, Kang and Xiang, Wei},
  journal={arXiv preprint arXiv:2303.03808},
  year={2023}
}

@article{cai2023neuda,
  title={NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction},
  author={Cai, Bowen and Huang, Jinchi and Jia, Rongfei and Lv, Chengfei and Fu, Huan},
  journal={arXiv preprint arXiv:2303.02375},
  year={2023}
}

@article{bai2023self,
  title={Self-NeRF: A Self-Training Pipeline for Few-Shot Neural Radiance Fields},
  author={Bai, Jiayang and Huang, Letian and Gong, Wen and Guo, Jie and Guo, Yanwen},
  journal={arXiv preprint arXiv:2303.05775},
  year={2023}
}

@article{moreau2023crossfire,
  title={CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation},
  author={Moreau, Arthur and Piasco, Nathan and Bennehar, Moussab and Tsishkou, Dzmitry and Stanciulescu, Bogdan and de La Fortelle, Arnaud},
  journal={arXiv preprint arXiv:2303.04869},
  year={2023}
}

@article{zhang2023structural,
  title={Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction},
  author={Zhang, Mingfang and Wang, Jinglu and Li, Xiao and Huang, Yifei and Sato, Yoichi and Lu, Yan},
  journal={arXiv preprint arXiv:2303.05937},
  year={2023}
}

@article{zhou2023nerflix,
  title={NeRFLiX: High-Quality Neural View Synthesis by Learning a Degradation-Driven Inter-viewpoint MiXer},
  author={Zhou, Kun and Li, Wenbo and Wang, Yi and Hu, Tao and Jiang, Nianjuan and Han, Xiaoguang and Lu, Jiangbo},
  journal={arXiv preprint arXiv:2303.06919},
  year={2023}
}

@article{yang2022freenerf,
  author = {Jiawei Yang and Marco Pavone and Yue Wang},
  title = {FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization},
  journal = {Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR)},
  year = {2023},
}


@article{rojas2023re,
  title={Re-ReND: Real-time Rendering of NeRFs across Devices},
  author={Rojas, Sara and Zarzar, Jesus and Perez, Juan Camilo and Sanakoyeu, Artsiom and Thabet, Ali and Pumarola, Albert and Ghanem, Bernard},
  journal={arXiv preprint arXiv:2303.08717},
  year={2023}
}

@article{wimbauer2023behind,
  title={Behind the Scenes: Density Fields for Single View Reconstruction},
  author={Wimbauer, Felix and Yang, Nan and Rupprecht, Christian and Cremers, Daniel},
  journal={arXiv preprint arXiv:2301.07668},
  year={2023}
}


@article{meng2023neat,
  title={NeAT: Learning Neural Implicit Surfaces with Arbitrary Topologies from Multi-view Images},
  author={Meng, Xiaoxu and Chen, Weikai and Yang, Bo},
  journal={arXiv preprint arXiv:2303.12012},
  year={2023}
}

@inproceedings{choi2023CVPR,
  author  ={Changwoon Choi and Sang Min Kim and Young Min Kim},
  title   ={Balanced Spherical Grid for Egocentric View Synthesis},
  booktitle ={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
  month   ={June},
  year    ={2023},
  pages   ={TBD},
}

@article{liang2023envidr,
  title={ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting},
  author={Liang, Ruofan and Chen, Huiting and Li, Chunlin and Chen, Fan and Panneer, Selvakumar and Vijaykumar, Nandita},
  journal={arXiv preprint arXiv:2303.13022},
  year={2023}
}

@inproceedings{meuleman2023localrf,
  author    = {Meuleman, Andreas and Liu, Yu-Lun and Gao, Chen and Huang, Jia-Bin and Kim, Changil and Kim, Min H. and Kopf, Johannes},
  title     = {Progressively Optimized Local Radiance Fields for Robust View Synthesis},
  booktitle = {CVPR},
  year      = {2023},
}

@misc{tang2023ablenerf,
      title={ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for Neural Radiance Field}, 
      author={Zhe Jun Tang and Tat-Jen Cham and Haiyu Zhao},
      year={2023},
      eprint={2303.13817},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

  @misc{hou2023neudf,
      title={NeUDF: Learning Unsigned Distance Fields from Multi-view Images for Reconstructing Non-watertight Models}, 
      author={Fei Hou and Jukai Deng and Xuhui Chen and Wencheng Wang and Ying He},
      year={2023},
      eprint={2303.15368},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jain2023enhanced,
      title={Enhanced Stable View Synthesis}, 
      author={Nishant Jain and Suryansh Kumar and Luc Van Gool},
      year={2023},
      eprint={2303.17094},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
	    
@misc{zhu2023vdnnerf,
      title={VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence Normalization}, 
      author={Bingfan Zhu and Yanchao Yang and Xulong Wang and Youyi Zheng and Leonidas Guibas},
      year={2023},
      eprint={2303.17968},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhang2023nemf,
      title={NeMF: Inverse Volume Rendering with Neural Microflake Field}, 
      author={Youjia Zhang and Teng Xu and Junqing Yu and Yuteng Ye and Junle Wang and Yanqing Jing and Jingyi Yu and Wei Yang},
      year={2023},
      eprint={2304.00782},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{Liu23NeUDF,
    author = Liu, Yu-Tao and Wang, Li and Yang, Jie and Chen, Weikai and Meng, Xiaoxu and Yang, Bo and Gao, Lin},
    title = {NeUDF: Leaning Neural Unsigned Distance Fields with Volume Rendering},
    booktitle={Computer Vision and Pattern Recognition (CVPR)},
    year = {2023},
} 

@inproceedings{wan2023ndrf,
    title={Learning Neural Duplex Radiance Fields for Real-Time View Synthesis},
    author={Ziyu Wan and Christian Richardt and Aljaž Božič and Chao Li and Vijay Rengarajan and Seonghyeon Nam and Xiaoyu Xiang and Tuotuo Li and Bo Zhu and Rakesh Ranjan and Jing Liao},
    booktitle={CVPR},
    year={2023}
}

@misc{chang2023pointersect,
      title={Pointersect: Neural Rendering with Cloud-Ray Intersection}, 
      author={Jen-Hao Rick Chang and Wei-Yu Chen and Anurag Ranjan and Kwang Moo Yi and Oncel Tuzel},
      year={2023},
      eprint={2304.12390},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{yuan2023nslfol,
      title={NSLF-OL: Online Learning of Neural Surface Light Fields alongside Real-time Incremental 3D Reconstruction}, 
      author={Yijun Yuan and Andreas Nuchter},
      year={2023},
      eprint={2305.00282},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
%%PointCloud
@article{huang2022boosting,
  title={Boosting Point Clouds Rendering via Radiance Mapping},
  author={Huang, Xiaoyang and Zhang, Yi and Ni, Bingbing and Li, Teng and Chen, Kai and Zhang, Wenjun},
  journal={arXiv preprint arXiv:2210.15107},
  year={2022}
}

%%Indoor
@article{zhu20232,
  title={I2-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs},
  author={Zhu, Jingsen and Huo, Yuchi and Ye, Qi and Luan, Fujun and Li, Jifan and Xi, Dianbing and Wang, Lisha and Tang, Rui and Hua, Wei and Bao, Hujun and others},
  journal={arXiv preprint arXiv:2303.07634},
  year={2023}
}

@article{liang2023helixsurf,
  title={HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization},
  author={Liang, Zhihao and Huang, Zhangjin and Ding, Changxing and Jia, Kui},
  journal={arXiv preprint arXiv:2302.14340},
  year={2023}
}

%%Large-scale
@article{zhang2023efficient,
  title={Efficient Large-scale Scene Representation with a Hybrid of High-resolution Grid and Plane Features},
  author={Zhang, Yuqi and Chen, Guanying and Cui, Shuguang},
  journal={arXiv preprint arXiv:2303.03003},
  year={2023}
}

@misc{xu2023gridguided,
  title={Grid-guided Neural Radiance Fields for Large Urban Scenes}, 
  author={Linning Xu and Yuanbo Xiangli and Sida Peng and Xingang Pan and Nanxuan Zhao and Christian Theobalt and Bo Dai and Dahua Lin},
  year={2023},
  eprint={2303.14001},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@inproceedings{mi2023switchnerf,
  title={Switch-NeRF: Learning Scene Decomposition with Mixture of Experts for Large-scale Neural Radiance Fields},
  author={Zhenxing Mi and Dan Xu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://openreview.net/forum?id=PQ2zoIZqvm}
}

%%Fourier
@article{wu2022neural,
  title={Neural Fourier Filter Bank},
  author={Wu, Zhijie and Jin, Yuhe and Moo Yi, Kwang},
  journal={arXiv e-prints},
  pages={arXiv--2212},
  year={2022}
}

@article{tancik2020fourfeat,
  title={Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
  author={Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
  journal={NeurIPS},
  year={2020}
}

%%poses
@article{levy2023melon,
  title={MELON: NeRF with Unposed Images Using Equivalence Class Estimation},
  author={Levy, Axel and Matthews, Mark and Sela, Matan and Wetzstein, Gordon and Lagun, Dmitry},
  journal={arXiv preprint arXiv:2303.08096},
  year={2023}
}

@inproceedings{bian2022nopenerf,
  author    = {Wenjing Bian and Zirui Wang and Kejie Li and Jiawang Bian and Victor Adrian Prisacariu},
  title     = {NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior},
  journal   = {CVPR},
  year      = {2023}
}
	
@article{sinha2022sparsepose,
  title={SparsePose: Sparse-View Camera Pose Regression and Refinement},
  author={Sinha, Samarth and Zhang, Jason Y and Tagliasacchi, Andrea and Gilitschenski, Igor and Lindell, David B},
  journal={arXiv preprint arXiv:2211.16991},
  year={2022}
}

@article{truong2022sparf,
  title={SPARF: Neural Radiance Fields from Sparse and Noisy Poses},
  author={Truong, Prune and Rakotosaona, Marie-Julie and Manhardt, Fabian and Tombari, Federico},
  journal={arXiv preprint arXiv:2211.11738},
  year={2022}
}

%% geometry
@article{kulhanek2023tetranerf,
  title={{T}etra-{NeRF}: Representing Neural Radiance Fields Using Tetrahedra},
  author={Kulhanek, Jonas and Sattler, Torsten},
  journal={arXiv preprint arXiv:2304.09987},
  year={2023},
}

@inproceedings{wang2023fegr,
  title = {Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes}, 
  author = {Zian Wang and Tianchang Shen and Jun Gao and Shengyu Huang and Jacob Munkberg 
  and Jon Hasselgren and Zan Gojcic and Wenzheng Chen and Sanja Fidler},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2023}
}

@misc{fang2023evaluate,
  title={Evaluate Geometry of Radiance Field with Low-frequency Color Prior}, 
  author={Qihang Fang and Yafei Song and Keqiang Li and Li Shen and Huaiyu Wu and Gang Xiong and Liefeng Bo},
  year={2023},
  eprint={2304.04351},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@misc{yang2023nerfvs,
  title={NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds}, 
  author={Chen Yang and Peihao Li and Zanwei Zhou and Shanxin Yuan and Bingbing Liu and Xiaokang Yang and Weichao Qiu and Wei Shen},
  year={2023},
  eprint={2304.06287},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@article{rakotosaona2023nerfmeshing,
  title={NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes},
  author={Rakotosaona, Marie-Julie and Manhardt, Fabian and Arroyo, Diego Martin and Niemeyer, Michael and Kundu, Abhijit and Tombari, Federico},
  journal={arXiv preprint arXiv:2303.09431},
  year={2023}
}

@article{10.1145/3528223.3530140,
  author = {Matveev, Albert and Rakhimov, Ruslan and Artemov, Alexey and Bobrovskikh, Gleb and Egiazarian, Vage and Bogomolov, Emil and Panozzo, Daniele and Zorin, Denis and Burnaev, Evgeny},
  title = {DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes},
  year = {2022},
  issue_date = {July 2022},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {41},
  number = {4},
  issn = {0730-0301},
  url = {https://doi.org/10.1145/3528223.3530140},
  doi = {10.1145/3528223.3530140},
  journal = {ACM Trans. Graph.},
  month = {jul},
  articleno = {108},
  numpages = {22},
  keywords = {curve extraction, sharp geometric features, deep learning}
}

@article{zou2022mononeuralfusion,
  title={MonoNeuralFusion: Online Monocular Neural 3D Reconstruction with Geometric Priors},
  author={Zou, Zi-Xin and Huang, Shi-Sheng and Cao, Yan-Pei and Mu, Tai-Jiang and Shan, Ying and Fu, Hongbo},
  journal={arXiv preprint arXiv:2209.15153},
  year={2022}
}


%% depth
@inproceedings{uy-scade-cvpr23,
  title = {SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates},
  author = {Mikaela Angelina Uy and Ricardo Martin-Brualla and Leonidas Guibas and Ke Li},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2023}
}
  
@article{wang2023sparsenerf,
  title={SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis},
  author={Guangcong and Zhaoxi Chen and Chen Change Loy and Ziwei Liu},
  journal={Technical Report},
  year={2023}
}

@inproceedings{bae2022irondepth,
  title={IronDepth: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty},
  author={Bae, Gwangbin and Budvytis, Ignas and Cipolla, Roberto},
  booktitle={British Machine Vision Conference (BMVC)},
  year={2022}
}

%%stereo
@inproceedings{du2023cross,
  title={Learning to Render Novel Views from Wide-Baseline Stereo Pairs},
  author={Du, Yilun and Smith, Cameron and Tewari, Ayush and Sitzmann, Vincent},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2023}
}

@inproceedings{Tosi2023CVPR,
  author    = {Tosi, Fabio and Tonioni, Alessio and De Gregorio, Daniele and Poggi, Matteo},
  title     = {NeRF-Supervised Deep Stereo},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2023}
}

%% visual challenges
@inproceedings{dai2023hybrid,
  title={Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur},
  author={Dai, Peng and Zhang, Yinda and Yu, Xin and Lyu, Xiaoyang and Qi, Xiaojuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2023}
}

@misc{levy2023seathrunerf,
  title={SeaThru-NeRF: Neural Radiance Fields in Scattering Media}, 
  author={Deborah Levy and Amit Peleg and Naama Pearl and Dan Rosenbaum and Derya Akkaynak and Simon Korman and Tali Treibitz},
  year={2023},
  eprint={2304.07743},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@misc{qiu2023looking,
  title={Looking Through the Glass: Neural Surface Reconstruction Against High Specular Reflections}, 
  author={Jiaxiong Qiu and Peng-Tao Jiang and Yifan Zhu and Ze-Xin Yin and Ming-Ming Cheng and Bo Ren},
  year={2023},
  eprint={2304.08706},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@inproceedings{Nerfbusters2023,
  title = {Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs},
  author = {Frederik Warburg* and Ethan Weber* and Matthew Tancik and Aleksander Hołyński and Angjoo Kanazawa},
  journal = {arXiv preprint},
  year = {2023},
}

@misc{tong2023seeing,
  title={Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container}, 
  author={Jinguang Tong and Sundaram Muthu and Fahira Afzal Maken and Chuong Nguyen and Hongdong Li},
  year={2023},
  eprint={2303.13805},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@misc{hu2023point2pix,
  title={Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields}, 
  author={Tao Hu and Xiaogang Xu and Shu Liu and Jiaya Jia},
  year={2023},
  eprint={2303.16482},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@misc{wang2022badnerf,
  title={BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields}, 
  author={Peng Wang and Lingzhe Zhao and Ruijie Ma and Peidong Liu},
  year={2022},
  eprint={2211.12853},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@misc{chen2023dehazenerf,
  title={DehazeNeRF: Multiple Image Haze Removal and 3D Shape Reconstruction using Neural Radiance Fields}, 
  author={Wei-Ting Chen and Wang Yifan and Sy-Yen Kuo and Gordon Wetzstein},
  year={2023},
  eprint={2303.11364},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@article{cui2023aleth,
  title={Aleth-NeRF: Low-light Condition View Synthesis with Concealing Fields},
  author={Cui, Ziteng and Gu, Lin and Sun, Xiao and Qiao, Yu and Harada, Tatsuya},
  journal={arXiv preprint arXiv:2303.05807},
  year={2023}
}

@article{wu2023alpha,
  title={$$\backslash$alpha $ Surf: Implicit Surface Reconstruction for Semi-Transparent and Thin Objects with Decoupled Geometry and Opacity},
  author={Wu, Tianhao and Liang, Hanxue and Zhong, Fangcheng and Riegler, Gernot and Vainer, Shimon and Oztireli, Cengiz},
  journal={arXiv preprint arXiv:2303.10083},
  year={2023}
}

@article{lee2023extremenerf,
  title={ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained Illumination},
  author={Lee, SeokYeong and Choi, JunYong and Kim, Seungryong and Kim, Ig-Jae and Cho, Junghyun},
  journal={arXiv preprint arXiv:2303.11728},
  year={2023}
}


@article{chen2023dehazenerf,
  title={DehazeNeRF: Multiple Image Haze Removal and 3D Shape Reconstruction using Neural Radiance Fields},
  author={Chen, Wei-Ting and Yifan, Wang and Kuo, Sy-Yen and Wetzstein, Gordon},
  journal={arXiv preprint arXiv:2303.11364},
  year={2023}
}

@inproceedings{low2022minimal,
  title={Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion},
  author={Low, Weng Fei and Lee, Gim Hee},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part II},
  pages={465--481},
  year={2022},
  organization={Springer}
}

@article{ge2023ref,
  title={Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection},
  author={Ge, Wenhang and Hu, Tao and Zhao, Haoyu and Liu, Shu and Chen, Ying-Cong},
  journal={arXiv preprint arXiv:2303.10840},
  year={2023}
}

@incollection{pan2022sampling,
  title={Sampling Neural Radiance Fields for Refractive Objects},
  author={Pan, Jen-I and Su, Jheng-Wei and Hsiao, Kai-Wen and Yen, Ting-Yu and Chu, Hung-Kuo},
  booktitle={SIGGRAPH Asia 2022 Technical Communications},
  pages={1--4},
  year={2022}
}